"""
ê°ì„± ë¶„ì„ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

ìˆ˜ì • ì‚¬í•­:
  - ZeroDivisionError ë°©ì§€ ë¡œì§ ì¶”ê°€ (ë°ì´í„° 0ê°œì¼ ë•Œ ì¤‘ë‹¨)
  - ëª¨ë¸ ì €ì¥ íŒŒì¼ëª… ë³€ê²½: logistic_regression_sentiment.joblib
  - NaN ê°’ ì²˜ë¦¬ë¥¼ ìœ„í•œ pd.notna() ë„ì…
"""

import os
import pandas as pd
import numpy as np
import joblib
import glob
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import (
    RandomForestClassifier,
    VotingClassifier,
    StackingClassifier,
)
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
    precision_recall_curve,
    average_precision_score,
    f1_score,
    matthews_corrcoef,
    precision_recall_fscore_support,
    precision_score,
    recall_score,
)
import matplotlib.pyplot as plt
import seaborn as sns

# í•œê¸€ í°íŠ¸ ì„¤ì •
from matplotlib import font_manager, rc
import platform
import sys

# í™˜ê²½ ê°ì§€ ìœ í‹¸ë¦¬í‹° import
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))
from utils.environment import is_colab

if platform.system() == "Windows":
    plt.rc("font", family="Malgun Gothic")
    KOREAN_FONT = "Malgun Gothic"
elif platform.system() == "Darwin":  # macOS
    plt.rc("font", family="AppleGothic")
    KOREAN_FONT = "AppleGothic"
else:  # ë¦¬ëˆ…ìŠ¤ (ì˜ˆ: Google Colab, Ubuntu)
    plt.rc("font", family="NanumGothic")
    KOREAN_FONT = "NanumGothic"

plt.rcParams["axes.unicode_minus"] = False  # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€

# ========== í•™ìŠµí•  ì¡°í•© ì„ íƒ ==========
# 1) ë²¡í„° íƒ€ì… ì„ íƒ (Noneì´ë©´ ì „ë¶€ ì‚¬ìš©)
#    ì‚¬ìš© ê°€ëŠ¥: "word2vec_sentiment", "bert_sentiment", "roberta_sentiment", "koelectra_sentiment"
VECTOR_TYPES_TO_USE = ["roberta_sentiment"]  # roberta_sentimentë§Œ ì‚¬ìš©
# VECTOR_TYPES_TO_USE = None  # ì „ë¶€ ì‚¬ìš©í•˜ë ¤ë©´ None

# 2) ML ëª¨ë¸ ì„ íƒ
#    ì‚¬ìš© ê°€ëŠ¥: "Logistic", "RandomForest", "DecisionTree", "XGBoost", "LightGBM", "SVM", "Voting", "Stacking"
ML_MODELS_TO_USE = [
    # "Logistic",
    # "RandomForest",
    # "DecisionTree",
    # "XGBoost",
    "LightGBM",
    # "SVM",
    # "Voting",
    # "Stacking",
]


def load_review_data(partitioned_reviews_dir, finetune_ids_path=None):
    print("======================================================================")
    print(f"ì „ì²˜ë¦¬ëœ ë¦¬ë·° ë°ì´í„° ë¡œë“œ ì¤‘: {partitioned_reviews_dir}")

    # Hive íŒŒí‹°ì…”ë‹: category=*/data.parquet íŒ¨í„´
    parquet_files = glob.glob(
        os.path.join(partitioned_reviews_dir, "category=*", "data.parquet")
    )

    if not parquet_files:
        print("[ì˜¤ë¥˜] Parquet íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    # íŒŒì¸íŠœë‹ì— ì‚¬ìš©ëœ ID ë¡œë“œ (ìˆëŠ” ê²½ìš°)
    finetune_ids = set()
    if finetune_ids_path and os.path.exists(finetune_ids_path):
        print(f"\níŒŒì¸íŠœë‹ ì‚¬ìš© ID ë¡œë“œ ì¤‘: {finetune_ids_path}")
        finetune_df = pd.read_csv(finetune_ids_path)
        # (product_id, id) íŠœí”Œë¡œ ì €ì¥
        finetune_ids = set(zip(finetune_df["product_id"], finetune_df["id"]))
        print(f"âœ“ ì œì™¸í•  ID: {len(finetune_ids):,}ê°œ")

    all_reviews = []
    total_loaded = 0
    total_excluded = 0

    for file_path in parquet_files:
        try:
            df = pd.read_parquet(file_path)
            category = os.path.basename(os.path.dirname(file_path)).replace(
                "category=", ""
            )
            total_loaded += len(df)

            # íŒŒì¸íŠœë‹ ì‚¬ìš© ID ì œì™¸
            if finetune_ids:
                before_count = len(df)
                # product_idì™€ idê°€ ëª¨ë‘ ìˆëŠ” í–‰ë§Œ í•„í„°ë§
                df = df[
                    ~df.apply(
                        lambda row: (row.get("product_id"), row.get("id"))
                        in finetune_ids,
                        axis=1,
                    )
                ]
                excluded = before_count - len(df)
                total_excluded += excluded
                print(f"  - {category}: {len(df):,}ê°œ ë¦¬ë·° (ì œì™¸: {excluded:,}ê°œ)")
            else:
                print(f"  - {category}: {len(df):,}ê°œ ë¦¬ë·°")

            all_reviews.extend(df.to_dict("records"))
        except Exception as e:
            print(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {file_path} - {e}")

    print(f"\nâœ“ ì´ ë¡œë“œ: {total_loaded:,}ê°œ")
    if finetune_ids:
        print(f"âœ“ íŒŒì¸íŠœë‹ ID ì œì™¸: {total_excluded:,}ê°œ")
        print(f"âœ“ ML í•™ìŠµìš© ë°ì´í„°: {len(all_reviews):,}ê°œ")
    else:
        print(f"âœ“ ì´ {len(all_reviews):,}ê°œ ë¦¬ë·° ë¡œë“œ ì™„ë£Œ")
    return all_reviews


def prepare_training_data(reviews):
    print("\ní•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€
    available_models = set()
    if reviews:
        sample = reviews[0]
        for key in sample.keys():
            # word2vec_sentiment, bert_sentiment, roberta_sentiment, koelectra_sentiment ë“± ë²¡í„° í•„ë“œ ê°ì§€
            if (
                key
                in [
                    "word2vec_sentiment",
                    "bert_sentiment",
                    "roberta_sentiment",
                    "koelectra_sentiment",
                ]
                and sample.get(key) is not None
            ):
                available_models.add(key)

    print(f"\n[ê°ì§€ëœ ëª¨ë¸ íƒ€ì…]")
    print(f"  - ì‚¬ìš© ê°€ëŠ¥: {sorted(available_models)}")

    # ëª¨ë¸ë³„ ë°ì´í„° ì €ì¥
    model_data = {model: {"X": [], "y": []} for model in available_models}

    # ë°ì´í„° ìƒ˜í”Œë¡œ êµ¬ì¡° í™•ì¸
    if reviews:
        sample = reviews[0]
        print(f"\n[ë°ì´í„° êµ¬ì¡° í™•ì¸]")
        print(f"  - ì „ì²´ í‚¤: {list(sample.keys())}")
        print(f"  - label ì¡´ì¬: {'label' in sample}, ê°’: {sample.get('label')}")

        for model_name in available_models:
            val = sample.get(model_name)
            if val is not None:
                print(
                    f"  - {model_name} íƒ€ì…: {type(val)}, ê¸¸ì´: {len(val) if hasattr(val, '__len__') else 'N/A'}"
                )

        # ì²˜ìŒ 100ê°œ ìƒ˜í”Œì—ì„œ í†µê³„
        label_count = sum(1 for r in reviews[:100] if pd.notna(r.get("label")))
        print(f"\n[ì²˜ìŒ 100ê°œ ìƒ˜í”Œ í™•ì¸]")
        print(f"  - label ìˆëŠ” ë¦¬ë·°: {label_count}ê°œ")
        for model_name in available_models:
            count = sum(1 for r in reviews[:100] if r.get(model_name) is not None)
            print(f"  - {model_name} ìˆëŠ” ë¦¬ë·°: {count}ê°œ")

    # ê° ë²¡í„° íƒ€ì…ë³„ë¡œ ë°ì´í„° ìˆ˜ì§‘
    for review in reviews:
        label = review.get("label")

        # labelì´ ìœ íš¨í•œì§€ í™•ì¸
        if not pd.notna(label):
            continue

        # ê° ëª¨ë¸ì˜ ë²¡í„° ìˆ˜ì§‘
        for model_name in available_models:
            vec = review.get(model_name)
            if vec is not None and isinstance(vec, (list, np.ndarray)) and len(vec) > 0:
                model_data[model_name]["X"].append(np.array(vec))
                model_data[model_name]["y"].append(int(label))

    # ê²°ê³¼ ì¶œë ¥
    results = {}
    for model_name in sorted(available_models):
        X = model_data[model_name]["X"]
        y = model_data[model_name]["y"]
        count = len(y)

        print(f"\nâœ“ {model_name.upper()} ë°ì´í„°: {count:,}ê°œ")
        if count > 0:
            pos = sum(y)
            neg = count - pos
            print(f"  - ê¸ì •: {pos:,}ê°œ ({pos/count*100:.1f}%)")
            print(f"  - ë¶€ì •: {neg:,}ê°œ ({neg/count*100:.1f}%)")
            print(f"  - ë²¡í„° ì°¨ì›: {len(X[0])}")
            results[model_name] = (np.array(X), np.array(y))
        else:
            results[model_name] = (np.array([]), np.array([]))

    return results


def get_model_dictionary():
    """ë¹„êµí•  ML ëª¨ë¸ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤."""
    lr = LogisticRegression(max_iter=1000, class_weight="balanced", random_state=42)
    rf = RandomForestClassifier(
        n_estimators=100, class_weight="balanced", random_state=42, n_jobs=-1
    )
    dt = DecisionTreeClassifier(class_weight="balanced", random_state=42)
    xgb = XGBClassifier(eval_metric="logloss", random_state=42, n_jobs=-1)
    lgbm = LGBMClassifier(
        class_weight="balanced", random_state=42, n_jobs=-1, verbose=-1
    )
    svc = SVC(probability=True, class_weight="balanced", random_state=42)

    # ì•™ìƒë¸” ëª¨ë¸ ì •ì˜
    estimators = [("lr", lr), ("rf", rf), ("xgb", xgb)]
    voting = VotingClassifier(estimators=estimators, voting="soft", n_jobs=-1)
    stacking = StackingClassifier(
        estimators=estimators, final_estimator=LogisticRegression(), n_jobs=-1
    )

    return {
        "Logistic": lr,
        "RandomForest": rf,
        "DecisionTree": dt,
        "XGBoost": xgb,
        "LightGBM": lgbm,
        "SVM": svc,
        "Voting": voting,
        "Stacking": stacking,
    }


def train_model(X_train, y_train, ml_model=None):
    """ëª¨ë¸ í•™ìŠµ

    Args:
        X_train: í•™ìŠµ ë°ì´í„°
        y_train: í•™ìŠµ ë ˆì´ë¸”
        ml_model: ì‚¬ìš©í•  ML ëª¨ë¸ (Noneì´ë©´ ê¸°ë³¸ LogisticRegression)
    """
    import time

    start_time = time.time()

    if ml_model is None:
        # ê¸°ë³¸ê°’: Logistic Regression
        model = LogisticRegression(
            max_iter=1000, random_state=42, class_weight="balanced"
        )
        print("\n[Logistic Regression] ëª¨ë¸ í•™ìŠµ ì¤‘...")
    else:
        model = ml_model
        model_name = type(model).__name__
        print(f"\n[{model_name}] ëª¨ë¸ í•™ìŠµ ì¤‘...")

    model.fit(X_train, y_train)

    train_time = time.time() - start_time
    print(f"âœ“ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ({train_time:.1f}ì´ˆ)")
    return model, train_time


def evaluate_model(model, X_test, y_test, output_dir, model_name="model"):
    print("\nëª¨ë¸ í‰ê°€ ì¤‘...")
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]  # ê¸ì • í´ë˜ìŠ¤ í™•ë¥ 

    # average=Noneìœ¼ë¡œ ê° í´ë˜ìŠ¤(0:ë¶€ì •, 1:ê¸ì •)ë³„ ì ìˆ˜ë¥¼ ì–»ìŒ
    precision_per_class, recall_per_class, f1_per_class, support_per_class = (
        precision_recall_fscore_support(y_test, y_pred, average=None, labels=[0, 1])
    )

    # ============ ê¸°ë³¸ ë©”íŠ¸ë¦­ ============
    accuracy = accuracy_score(y_test, y_pred)
    f1_macro = f1_score(y_test, y_pred, average="macro")  # í´ë˜ìŠ¤ í‰ê· 
    f1_weighted = f1_score(y_test, y_pred, average="weighted")  # ê°€ì¤‘ í‰ê· 
    mcc = matthews_corrcoef(y_test, y_pred)

    print("\n" + "=" * 70)
    print("ê¸°ë³¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­")
    print("=" * 70)
    print(f"ì •í™•ë„ (Accuracy):         {accuracy:.4f}")
    print(f"F1 Score (Macro Avg):      {f1_macro:.4f}")
    print(f"F1 Score (Weighted Avg):   {f1_weighted:.4f}")
    print(f"Matthews Corr Coef:        {mcc:.4f}")

    # ============ í´ë˜ìŠ¤ë³„ ìƒì„¸ ì„±ëŠ¥ (í•µì‹¬!) ============
    print("\n" + "=" * 70)
    print("í´ë˜ìŠ¤ë³„ ìƒì„¸ ì„±ëŠ¥ (Class-wise Performance)")
    print("=" * 70)
    print(
        f"{'í´ë˜ìŠ¤':<10} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}"
    )
    print("-" * 70)
    print(
        f"{'ë¶€ì •(0)':<10} {precision_per_class[0]:<12.4f} {recall_per_class[0]:<12.4f} {f1_per_class[0]:<12.4f} {support_per_class[0]:<10,}"
    )
    print(
        f"{'ê¸ì •(1)':<10} {precision_per_class[1]:<12.4f} {recall_per_class[1]:<12.4f} {f1_per_class[1]:<12.4f} {support_per_class[1]:<10,}"
    )
    print("\n[í•´ì„]")
    print(
        f"  â€¢ ë¶€ì • ë¦¬ë·° F1: {f1_per_class[0]:.4f} - ë¶€ì • ë¦¬ë·°ë¥¼ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•˜ëŠ”ê°€"
    )
    print(
        f"  â€¢ ê¸ì • ë¦¬ë·° F1: {f1_per_class[1]:.4f} - ê¸ì • ë¦¬ë·°ë¥¼ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•˜ëŠ”ê°€"
    )
    print(
        f"  â€¢ ë¶€ì • Recall: {recall_per_class[0]:.4f} - ì‹¤ì œ ë¶€ì • ë¦¬ë·° ì¤‘ ëª‡ %ë¥¼ ì°¾ì•„ëƒˆëŠ”ê°€"
    )
    print(
        f"  â€¢ ê¸ì • Recall: {recall_per_class[1]:.4f} - ì‹¤ì œ ê¸ì • ë¦¬ë·° ì¤‘ ëª‡ %ë¥¼ ì°¾ì•„ëƒˆëŠ”ê°€"
    )

    # ============ ë¶„ë¥˜ ë¦¬í¬íŠ¸ ============
    print("\n" + "=" * 70)
    print("ë¶„ë¥˜ ë¦¬í¬íŠ¸ (Classification Report)")
    print("=" * 70)
    print(classification_report(y_test, y_pred, target_names=["ë¶€ì •(0)", "ê¸ì •(1)"]))

    # ============ í˜¼ë™ í–‰ë ¬ ìƒì„¸ ë¶„ì„ ============
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()

    print("\n" + "=" * 70)
    print("í˜¼ë™ í–‰ë ¬ ìƒì„¸ ë¶„ì„")
    print("=" * 70)
    print(f"True Negative (TN):    {tn:,}ê°œ - ë¶€ì •ì„ ë¶€ì •ìœ¼ë¡œ ë§ì¶¤")
    print(f"False Positive (FP):   {fp:,}ê°œ - ë¶€ì •ì„ ê¸ì •ìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡")
    print(f"False Negative (FN):   {fn:,}ê°œ - ê¸ì •ì„ ë¶€ì •ìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡")
    print(f"True Positive (TP):    {tp:,}ê°œ - ê¸ì •ì„ ê¸ì •ìœ¼ë¡œ ë§ì¶¤")
    print(f"\nSpecificity (íŠ¹ì´ë„):  {tn/(tn+fp):.4f} - ë¶€ì • í´ë˜ìŠ¤ íƒì§€ ì„±ëŠ¥")
    print(f"Sensitivity (ë¯¼ê°ë„):  {tp/(tp+fn):.4f} - ê¸ì • í´ë˜ìŠ¤ íƒì§€ ì„±ëŠ¥")

    # ============ ROC Curve & AUC ============
    fpr, tpr, thresholds_roc = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)

    print("\n" + "=" * 70)
    print("ROC & AUC")
    print("=" * 70)
    print(f"AUC (Area Under ROC): {roc_auc:.4f}")

    # ============ Precision-Recall Curve ============
    precision, recall, thresholds_pr = precision_recall_curve(y_test, y_proba)
    avg_precision = average_precision_score(y_test, y_proba)

    print(f"Average Precision:    {avg_precision:.4f}")

    # ============ ì‹œê°í™” ============
    fig, axes = plt.subplots(2, 3, figsize=(20, 12))

    # 1. Confusion Matrix
    sns.heatmap(
        cm,
        annot=True,
        fmt="d",
        cmap="Blues",
        xticklabels=["ë¶€ì •", "ê¸ì •"],
        yticklabels=["ë¶€ì •", "ê¸ì •"],
        ax=axes[0, 0],
    )
    axes[0, 0].set_title("Confusion Matrix", fontsize=14, fontweight="bold")
    axes[0, 0].set_ylabel("ì‹¤ì œ", fontsize=12)
    axes[0, 0].set_xlabel("ì˜ˆì¸¡", fontsize=12)

    # 2. ROC Curve
    axes[0, 1].plot(
        fpr, tpr, color="darkorange", lw=2, label=f"ROC (AUC = {roc_auc:.3f})"
    )
    axes[0, 1].plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--", label="Random")
    axes[0, 1].set_xlim([0.0, 1.0])
    axes[0, 1].set_ylim([0.0, 1.05])
    axes[0, 1].set_xlabel("False Positive Rate", fontsize=12)
    axes[0, 1].set_ylabel("True Positive Rate", fontsize=12)
    axes[0, 1].set_title("ROC Curve", fontsize=14, fontweight="bold")
    axes[0, 1].legend(loc="lower right")
    axes[0, 1].grid(alpha=0.3)

    # 3. Precision-Recall Curve
    axes[1, 0].plot(
        recall, precision, color="blue", lw=2, label=f"PR (AP = {avg_precision:.3f})"
    )
    axes[1, 0].set_xlim([0.0, 1.0])
    axes[1, 0].set_ylim([0.0, 1.05])
    axes[1, 0].set_xlabel("Recall", fontsize=12)
    axes[1, 0].set_ylabel("Precision", fontsize=12)
    axes[1, 0].set_title("Precision-Recall Curve", fontsize=14, fontweight="bold")
    axes[1, 0].legend(loc="lower left")
    axes[1, 0].grid(alpha=0.3)

    # 4. í™•ë¥  ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    axes[1, 1].hist(
        y_proba[y_test == 0], bins=50, alpha=0.5, label="ë¶€ì •(ì‹¤ì œ)", color="red"
    )
    axes[1, 1].hist(
        y_proba[y_test == 1], bins=50, alpha=0.5, label="ê¸ì •(ì‹¤ì œ)", color="green"
    )
    axes[1, 1].set_xlabel("ì˜ˆì¸¡ í™•ë¥  (ê¸ì • í´ë˜ìŠ¤)", fontsize=12)
    axes[1, 1].set_ylabel("ë¹ˆë„", fontsize=12)
    axes[1, 1].set_title("ì˜ˆì¸¡ í™•ë¥  ë¶„í¬", fontsize=14, fontweight="bold")
    axes[1, 1].legend(loc="upper center")
    axes[1, 1].grid(alpha=0.3)

    # 5. ë¹ˆ ê³µê°„ í™œìš©
    axes[0, 2].axis("off")

    # 6. ë¹ˆ ê³µê°„ì— ë©”íŠ¸ë¦­ ìš”ì•½ í‘œì‹œ
    metrics_text = f"""ì„±ëŠ¥ ìš”ì•½
    
ì •í™•ë„: {accuracy:.4f}
F1 (Macro): {f1_macro:.4f}
F1 (Weighted): {f1_weighted:.4f}
MCC: {mcc:.4f}
AUC: {roc_auc:.4f}
Avg Precision: {avg_precision:.4f}

ë¶€ì • F1: {f1_per_class[0]:.4f}
ê¸ì • F1: {f1_per_class[1]:.4f}

TN: {tn:,}  FP: {fp:,}
FN: {fn:,}  TP: {tp:,}

Specificity: {tn/(tn+fp):.4f}
Sensitivity: {tp/(tp+fn):.4f}"""

    axes[1, 2].text(
        0.1,
        0.5,
        metrics_text,
        fontsize=11,
        verticalalignment="center",
        family=KOREAN_FONT,
        bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.3),
    )
    axes[1, 2].axis("off")

    plt.tight_layout()

    # íŒŒì¼ëª…ì— ëª¨ë¸ ì´ë¦„ í¬í•¨
    eval_path = os.path.join(output_dir, f"model_evaluation_{model_name}.png")
    plt.savefig(eval_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"\nâœ“ í‰ê°€ ê²°ê³¼ ì‹œê°í™” ì €ì¥: {eval_path}")

    # ============ ì„ê³„ê°’ ë¶„ì„ ============
    print("\n" + "=" * 70)
    print("ì„ê³„ê°’ë³„ ì„±ëŠ¥ (ìƒìœ„ 5ê°œ)")
    print("=" * 70)
    print(f"{'Threshold':>10} {'Precision':>10} {'Recall':>10} {'F1-Score':>10}")
    print("-" * 70)

    # ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œ ì„±ëŠ¥ ê³„ì‚°
    threshold_candidates = [0.3, 0.4, 0.5, 0.6, 0.7]
    for threshold in threshold_candidates:
        y_pred_custom = (y_proba >= threshold).astype(int)
        prec = precision_score_custom(y_test, y_pred_custom)
        rec = recall_score_custom(y_test, y_pred_custom)
        f1_custom = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0
        print(f"{threshold:>10.2f} {prec:>10.4f} {rec:>10.4f} {f1_custom:>10.4f}")

    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜
    return {
        "accuracy": accuracy,
        "f1_macro": f1_macro,
        "f1_weighted": f1_weighted,
        "f1_neg": f1_per_class[0],
        "f1_pos": f1_per_class[1],
        "mcc": mcc,
        "auc": roc_auc,
        "avg_precision": avg_precision,
    }


def precision_score_custom(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    return tp / (tp + fp) if (tp + fp) > 0 else 0


def recall_score_custom(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))
    return tp / (tp + fn) if (tp + fn) > 0 else 0


def main():
    print("=" * 70)
    print(f"ê°ì„± ë¶„ì„ ëª¨ë¸ í•™ìŠµ (ì‚¬ìš© ëª¨ë¸: {', '.join(ML_MODELS_TO_USE)})")
    print("=" * 70)

    # ê²½ë¡œ ì„¤ì • (Colab í™˜ê²½ ê³ ë ¤)
    if is_colab():
        BASE_DIR = "/content"
        PROCESSED_DATA_DIR = os.path.join(BASE_DIR, "data/processed_data")
        MODEL_OUTPUT_DIR = os.path.join(BASE_DIR, "models")
        FINETUNE_IDS_PATH = os.path.join(BASE_DIR, "finetune_used_ids.csv")
    else:
        BASE_DIR = "./data"
        PROCESSED_DATA_DIR = "./data/processed_data"
        MODEL_OUTPUT_DIR = "./models"
        FINETUNE_IDS_PATH = os.path.join(BASE_DIR, "finetune_used_ids.csv")

    PARTITIONED_REVIEWS_DIR = os.path.join(PROCESSED_DATA_DIR, "partitioned_reviews")
    os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)

    # 1. ë°ì´í„° ë¡œë“œ (íŒŒì¸íŠœë‹ ì‚¬ìš© ID ì œì™¸)
    reviews = load_review_data(PARTITIONED_REVIEWS_DIR, FINETUNE_IDS_PATH)
    if not reviews:
        print("[ì¤‘ë‹¨] ë¡œë“œëœ ë¦¬ë·° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (ëª¨ë“  ëª¨ë¸ ìë™ ê°ì§€)
    model_data = prepare_training_data(reviews)
    if not model_data:
        print("\n[ì¤‘ë‹¨] í•™ìŠµ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•œ ê²°ê³¼ ì €ì¥
    performance_results = []

    # ML ëª¨ë¸ ë”•ì…”ë„ˆë¦¬ ê°€ì ¸ì˜¤ê¸°
    ml_models_dict = get_model_dictionary()

    # ì„ íƒëœ ML ëª¨ë¸ë§Œ í•„í„°ë§
    selected_ml_models = {
        name: model
        for name, model in ml_models_dict.items()
        if name in ML_MODELS_TO_USE
    }

    if not selected_ml_models:
        print("\n[ê²½ê³ ] ML_MODELS_TO_USEê°€ ë¹„ì–´ìˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ ëª¨ë¸ëª…ì…ë‹ˆë‹¤.")
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(ml_models_dict.keys())}")
        return

    print(f"\nì„ íƒëœ ML ëª¨ë¸: {list(selected_ml_models.keys())}")

    # ë²¡í„° íƒ€ì… í•„í„°ë§
    if VECTOR_TYPES_TO_USE is not None:
        available_vectors = {
            vname: vdata
            for vname, vdata in model_data.items()
            if vname in VECTOR_TYPES_TO_USE
        }
        if not available_vectors:
            print(f"\n[ê²½ê³ ] VECTOR_TYPES_TO_USEì— ì§€ì •ëœ ë²¡í„°ê°€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
            print(f"  - ìš”ì²­: {VECTOR_TYPES_TO_USE}")
            print(f"  - ì‚¬ìš© ê°€ëŠ¥: {list(model_data.keys())}")
            return
        print(f"ì„ íƒëœ ë²¡í„° íƒ€ì…: {list(available_vectors.keys())}")
    else:
        available_vectors = model_data
        print(f"ì „ì²´ ë²¡í„° íƒ€ì… ì‚¬ìš©: {list(available_vectors.keys())}")

    # í•™ìŠµí•  ì¡°í•© ê°œìˆ˜ ë¯¸ë¦¬ ê³„ì‚°
    total_combinations = len(available_vectors) * len(selected_ml_models)
    print(f"\nğŸ’¡ ì´ {total_combinations}ê°œ ì¡°í•© í•™ìŠµ ì˜ˆì •")
    print(f"   ({len(available_vectors)}ê°œ ë²¡í„° Ã— {len(selected_ml_models)}ê°œ ML ëª¨ë¸)")

    # 3. ê° ë²¡í„° íƒ€ì…ë³„ë¡œ í•™ìŠµ ë° í‰ê°€
    for vector_name in sorted(available_vectors.keys()):
        X, y = available_vectors[vector_name]

        if X.size == 0:
            print(f"\n[ê±´ë„ˆëœ€] {vector_name.upper()}: ë°ì´í„° ì—†ìŒ")
            continue

        print("\n" + "=" * 100)
        print(f"{vector_name.upper()} ë²¡í„° ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ")
        print("=" * 100)

        print("\në°ì´í„° ë¶„í•  ì¤‘...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        print(f"âœ“ í›ˆë ¨: {len(X_train):,}ê°œ / í…ŒìŠ¤íŠ¸: {len(X_test):,}ê°œ")

        # ê° ML ëª¨ë¸ë³„ë¡œ í•™ìŠµ
        for ml_model_name, ml_model in selected_ml_models.items():
            print("\n" + "-" * 80)
            print(f"[{vector_name.upper()}] Ã— [{ml_model_name}] ì¡°í•©")
            print("-" * 80)

            # ëª¨ë¸ í•™ìŠµ
            model, train_time = train_model(X_train, y_train, ml_model)

            # ëª¨ë¸ í‰ê°€
            combined_name = f"{vector_name}_{ml_model_name}"
            performance = evaluate_model(
                model,
                X_test,
                y_test,
                MODEL_OUTPUT_DIR,
                model_name=combined_name,
            )

            # ì„±ëŠ¥ ê²°ê³¼ ì €ì¥
            performance["vector_name"] = vector_name
            performance["ml_model_name"] = ml_model_name
            performance["combined_name"] = combined_name
            performance["train_time"] = train_time
            performance_results.append(performance)

            # ëª¨ë¸ ì €ì¥
            model_path = os.path.join(MODEL_OUTPUT_DIR, f"{combined_name}.joblib")
            joblib.dump(model, model_path)
            print(f"\nâœ“ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")

    # 4. ì„±ëŠ¥ ë¹„êµ í‘œ ì¶œë ¥
    if performance_results:
        print("\n" + "=" * 130)
        print("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (ë²¡í„° íƒ€ì… Ã— ML ëª¨ë¸)")
        print("=" * 130)

        # í—¤ë”
        header = f"{'Vector':<12} {'ML Model':<15} {'Accuracy':>9} {'F1 Macro':>9} {'F1 Neg':>9} {'F1 Pos':>9} {'AUC':>9} {'MCC':>9} {'Train Time':>12}"
        print(header)
        print("-" * 130)

        # MCC ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_results = sorted(
            performance_results, key=lambda x: x["mcc"], reverse=True
        )

        # ê° ëª¨ë¸ ê²°ê³¼
        for result in sorted_results:
            row = (
                f"{result['vector_name']:<12} "
                f"{result['ml_model_name']:<15} "
                f"{result['accuracy']:>9.4f} "
                f"{result['f1_macro']:>9.4f} "
                f"{result['f1_neg']:>9.4f} "
                f"{result['f1_pos']:>9.4f} "
                f"{result['auc']:>9.4f} "
                f"{result['mcc']:>9.4f} "
                f"{result['train_time']:>11.1f}s"
            )
            print(row)

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í‘œì‹œ
        best_acc = max(performance_results, key=lambda x: x["accuracy"])
        best_f1_macro = max(performance_results, key=lambda x: x["f1_macro"])
        best_f1_neg = max(performance_results, key=lambda x: x["f1_neg"])
        best_auc = max(performance_results, key=lambda x: x["auc"])
        best_mcc = max(performance_results, key=lambda x: x["mcc"])

        print("\n" + "-" * 130)
        print("ìµœê³  ì„±ëŠ¥:")
        print(
            f"  - Accuracy:  {best_acc['combined_name']} ({best_acc['accuracy']:.4f})"
        )
        print(
            f"  - F1 Macro:  {best_f1_macro['combined_name']} ({best_f1_macro['f1_macro']:.4f})"
        )
        print(
            f"  - F1 ë¶€ì •:   {best_f1_neg['combined_name']} ({best_f1_neg['f1_neg']:.4f})"
        )
        print(f"  - AUC:       {best_auc['combined_name']} ({best_auc['auc']:.4f})")
        print(
            f"  - MCC:       {best_mcc['combined_name']} ({best_mcc['mcc']:.4f}) â­ ì¶”ì²œ"
        )
        print("=" * 130)

    print("\n" + "=" * 70)
    print("í•™ìŠµ ì™„ë£Œ!")
    print(f"ì´ {len(performance_results)}ê°œ ëª¨ë¸ ì €ì¥ë¨:")
    for result in sorted_results[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
        print(
            f"  - {result['combined_name']}: sentiment_{result['combined_name']}.joblib"
        )
    if len(sorted_results) > 5:
        print(f"  ... ì™¸ {len(sorted_results) - 5}ê°œ")
    print("=" * 70)


if __name__ == "__main__":
    main()
