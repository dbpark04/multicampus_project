"""
ê°ì„± ë¶„ì„ ëª¨ë¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

ë‹¨ì¼ í…ìŠ¤íŠ¸ ë˜ëŠ” ë°°ì¹˜ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê°ì„± ì ìˆ˜ ì˜ˆì¸¡
"""

import sys
import os
import re
import joblib
import numpy as np
from konlpy.tag import Okt
from gensim.models import Word2Vec

# í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
okt = Okt()

# ì „ì—­ ë³€ìˆ˜ (ìºì‹±ìš©)
_model_word2vec = None
_model_bert = None
_stopwords = None
_w2v_model = None
_bert_vectorizer = None


def load_stopwords():
    """ë¶ˆìš©ì–´ ë¡œë“œ"""
    stopwords_path = os.path.join(
        os.path.dirname(__file__), "../preprocessing/stopwords-ko.txt"
    )
    if not os.path.exists(stopwords_path):
        raise FileNotFoundError(f"ë¶ˆìš©ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {stopwords_path}")

    with open(stopwords_path, "r", encoding="utf-8") as f:
        return set([line.strip() for line in f if line.strip()])


def load_model(vectorizer_type="word2vec"):
    """ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ

    Args:
        vectorizer_type (str): "word2vec" ë˜ëŠ” "bert"
    """
    model_filename = f"logistic_regression_sentiment_{vectorizer_type}.joblib"
    model_path = os.path.join(os.path.dirname(__file__), "../../models", model_filename)
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

    return joblib.load(model_path)


def load_word2vec_model():
    """Word2Vec ëª¨ë¸ ë¡œë“œ"""
    model_path = os.path.join(
        os.path.dirname(__file__), "../../models/word2vec_model.model"
    )
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Word2Vec ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

    return Word2Vec.load(model_path)


def load_bert_vectorizer():
    """BERT Vectorizer ë¡œë“œ"""
    import sys

    bert_path = os.path.join(os.path.dirname(__file__), "../preprocessing")
    if bert_path not in sys.path:
        sys.path.insert(0, bert_path)

    from bert_vectorizer import get_bert_vectorizer

    return get_bert_vectorizer("klue/bert-base")


def initialize(vectorizer_type="word2vec"):
    """ëª¨ë¸, ë¶ˆìš©ì–´, ë²¡í„°í™” ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ (í•œ ë²ˆë§Œ)

    Args:
        vectorizer_type (str): "word2vec" ë˜ëŠ” "bert"
    """
    global _model_word2vec, _model_bert, _stopwords, _w2v_model, _bert_vectorizer

    if vectorizer_type == "word2vec":
        if _model_word2vec is None:
            _model_word2vec = load_model("word2vec")
            print("âœ“ Word2Vec ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        if _w2v_model is None:
            _w2v_model = load_word2vec_model()
            print(f"âœ“ Word2Vec ë²¡í„°í™” ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            print(f"  - ì–´íœ˜ í¬ê¸°: {len(_w2v_model.wv):,}ê°œ")
            print(f"  - ë²¡í„° ì°¨ì›: {_w2v_model.vector_size}ì°¨ì›")

    elif vectorizer_type == "bert":
        if _model_bert is None:
            _model_bert = load_model("bert")
            print("âœ“ BERT ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        if _bert_vectorizer is None:
            _bert_vectorizer = load_bert_vectorizer()
            print(f"âœ“ BERT ë²¡í„°í™” ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    if _stopwords is None:
        _stopwords = load_stopwords()
        print(f"âœ“ ë¶ˆìš©ì–´ ë¡œë“œ ì™„ë£Œ: {len(_stopwords)}ê°œ")


def tokenize_text(text):
    """í…ìŠ¤íŠ¸ë¥¼ í† í°í™” (ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼í•œ ë°©ì‹)"""
    if _stopwords is None:
        initialize("word2vec")  # í† í°í™”ë§Œ í•„ìš”í•œ ê²½ìš° word2vec ì´ˆê¸°í™”

    if not isinstance(text, str) or not text.strip():
        return []

    # íŠ¹ìˆ˜ë¬¸ì ì œê±°, í•œê¸€/ìˆ«ìë§Œ ìœ ì§€
    clean_text = re.sub(r"[^ê°€-í£0-9\s]", " ", text)
    clean_text = re.sub(r"\s+", " ", clean_text).strip()

    # í˜•íƒœì†Œ ë¶„ì„ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ë§Œ)
    tokens = []
    for word, pos in okt.pos(clean_text, stem=True):
        if pos in ("Noun", "Verb", "Adjective") and word not in _stopwords:
            tokens.append(word)

    return tokens


def tokens_to_vector(tokens):
    """í† í°ì„ Word2Vec ë²¡í„°ë¡œ ë³€í™˜ (í‰ê· )"""
    if _w2v_model is None:
        initialize()

    if not tokens:
        return np.zeros(_w2v_model.vector_size)

    # Word2Vec ëª¨ë¸ì—ì„œ ê° í† í°ì˜ ë²¡í„°ë¥¼ ê°€ì ¸ì˜´
    valid_vectors = []
    for token in tokens:
        if token in _w2v_model.wv:
            valid_vectors.append(_w2v_model.wv[token])

    if not valid_vectors:
        # ë²¡í„°ê°€ ì—†ëŠ” ê²½ìš° 0 ë²¡í„° ë°˜í™˜
        return np.zeros(_w2v_model.vector_size)

    # í‰ê·  ë²¡í„° ë°˜í™˜
    return np.mean(valid_vectors, axis=0)


def predict_sentiment(text, verbose=False, vectorizer_type="word2vec"):
    """
    í…ìŠ¤íŠ¸ì˜ ê°ì„± ì ìˆ˜ ì˜ˆì¸¡

    Args:
        text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸
        verbose (bool): ìƒì„¸ ì •ë³´ ì¶œë ¥ ì—¬ë¶€
        vectorizer_type (str): "word2vec" ë˜ëŠ” "bert"

    Returns:
        float: ê¸ì • í™•ë¥  (0.0 ~ 1.0)
               1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê¸ì •ì 

    Examples:
        >>> score = predict_sentiment("ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”!")
        >>> print(f"ê°ì„± ì ìˆ˜: {score:.3f}")
    """
    global _model_word2vec, _model_bert, _w2v_model, _bert_vectorizer

    # ì´ˆê¸°í™”
    if vectorizer_type == "word2vec":
        if _model_word2vec is None or _w2v_model is None:
            initialize("word2vec")
        model = _model_word2vec
    else:
        if _model_bert is None or _bert_vectorizer is None:
            initialize("bert")
        model = _model_bert

    if verbose:
        print(f"\n[ì‚¬ìš© ëª¨ë¸: {vectorizer_type.upper()}]")
        print(f"ì›ë¬¸: {text}")

    # ë²¡í„°í™”
    if vectorizer_type == "word2vec":
        # 1. í† í°í™”
        tokens = tokenize_text(text)

        if verbose:
            print(f"í† í°: {tokens}")

        if not tokens:
            if verbose:
                print("âš ï¸ ìœ íš¨í•œ í† í°ì´ ì—†ìŠµë‹ˆë‹¤. ì¤‘ë¦½ ì ìˆ˜ ë°˜í™˜")
            return 0.5  # ì¤‘ë¦½

        # 2. Word2Vec ë²¡í„°í™”
        vector = tokens_to_vector(tokens)
    else:
        # BERT ë²¡í„°í™” (í† í°í™” ë¶ˆí•„ìš”, ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        vector = _bert_vectorizer.encode(text)

        if verbose:
            print(f"BERT ë²¡í„° ì°¨ì›: {len(vector)}")

    # 3. ê°ì„± ì ìˆ˜ ì˜ˆì¸¡
    proba = model.predict_proba([vector])[0][1]

    if verbose:
        print(f"ê°ì„± ì ìˆ˜: {proba:.4f}")
        sentiment_label = "ê¸ì •" if proba >= 0.6 else "ë¶€ì •" if proba <= 0.4 else "ì¤‘ë¦½"
        print(f"íŒì •: {sentiment_label}")

    return proba


def batch_predict(texts, show_progress=False, vectorizer_type="word2vec"):
    """
    ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì˜ˆì¸¡

    Args:
        texts (list): í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        show_progress (bool): ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€
        vectorizer_type (str): "word2vec" ë˜ëŠ” "bert"

    Returns:
        list: ê°ì„± ì ìˆ˜ ë¦¬ìŠ¤íŠ¸

    Examples:
        >>> texts = ["ì¢‹ì•„ìš”", "ë³„ë¡œì˜ˆìš”", "ê·¸ëƒ¥ ê·¸ë˜ìš”"]
        >>> scores = batch_predict(texts)
    """
    # ì´ˆê¸°í™”
    initialize(vectorizer_type)

    results = []
    iterator = enumerate(texts)

    if show_progress:
        try:
            from tqdm import tqdm

            iterator = tqdm(iterator, total=len(texts), desc="ê°ì„± ë¶„ì„")
        except ImportError:
            pass

    for i, text in iterator:
        score = predict_sentiment(text, verbose=False, vectorizer_type=vectorizer_type)
        results.append(score)

    return results


def get_sentiment_label(score, positive_threshold=0.6, negative_threshold=0.4):
    """
    ê°ì„± ì ìˆ˜ë¥¼ ë ˆì´ë¸”ë¡œ ë³€í™˜

    Args:
        score (float): ê°ì„± ì ìˆ˜ (0.0 ~ 1.0)
        positive_threshold (float): ê¸ì • íŒë‹¨ ì„ê³„ê°’
        negative_threshold (float): ë¶€ì • íŒë‹¨ ì„ê³„ê°’

    Returns:
        str: "ê¸ì •", "ë¶€ì •", "ì¤‘ë¦½"
    """
    if score >= positive_threshold:
        return "ê¸ì •"
    elif score <= negative_threshold:
        return "ë¶€ì •"
    else:
        return "ì¤‘ë¦½"


# ì˜ˆì‹œ ë¬¸ì¥
EXAMPLE_SENTENCES = [
    # ê¸ì • ë¦¬ë·°
    "ì´ ì œí’ˆ ì •ë§ ë§Œì¡±ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. ê°•ë ¥ ì¶”ì²œí•©ë‹ˆë‹¤!",
    "ê°€ì„±ë¹„ ìµœê³ ì˜ˆìš”. í’ˆì§ˆë„ ë„ˆë¬´ ì¢‹ê³  ë°°ì†¡ë„ ë¹¨ë¼ìš”.",
    "í”¼ë¶€ê°€ ì´‰ì´‰í•´ì§€ê³  ë°œë¦¼ì„±ë„ í›Œë¥­í•´ìš”. ì¬êµ¬ë§¤ ì˜ì‚¬ 100%",
    "í–¥ë„ ì¢‹ê³  ì‚¬ìš©ê°ì´ ë¶€ë“œëŸ¬ì›Œì„œ ë§¤ì¼ ì‚¬ìš©í•˜ê³  ìˆì–´ìš”.",
    "ë¯¼ê°í•œ í”¼ë¶€ì¸ë° ìê·¹ ì—†ì´ ì˜ ë§ì•„ìš”. ëŒ€ë°•",
    # ë¶€ì • ë¦¬ë·°
    "ì „í˜€ íš¨ê³¼ê°€ ì—†ë„¤ìš”. ëˆ ì•„ê¹ìŠµë‹ˆë‹¤.",
    "í”¼ë¶€ì— íŠ¸ëŸ¬ë¸”ì´ ìƒê²¼ì–´ìš”. ì‹¤ë§ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.",
    "ëƒ„ìƒˆê°€ ë„ˆë¬´ ê°•í•˜ê³  ë°œë¦¼ì„±ë„ ë³„ë¡œì˜ˆìš”.",
    "ê°€ê²© ëŒ€ë¹„ í’ˆì§ˆì´ ì‹¤ë§ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. ì¬êµ¬ë§¤ ì˜ì‚¬ ì—†ìŒ",
    "ë°°ì†¡ë„ ëŠ¦ê³  ì œí’ˆë„ ë³„ë¡œì…ë‹ˆë‹¤. ìµœì•…ì´ì—ìš”.",
    # ì¤‘ë¦½ ë¦¬ë·°
    "ê·¸ëƒ¥ í‰ë²”í•œ ì œí’ˆì´ì—ìš”. ë‚˜ì˜ì§€ë„ ì¢‹ì§€ë„ ì•Šì•„ìš”.",
    "ê°€ê²©ì€ ì €ë ´í•œë° íš¨ê³¼ëŠ” ëª¨ë¥´ê² ì–´ìš”.",
    "íŠ¹ë³„í•œ ëŠë‚Œì€ ì—†ì§€ë§Œ ì“¸ë§Œí•´ìš”.",
]


def main():
    """ë©”ì¸ í•¨ìˆ˜ - í…ŒìŠ¤íŠ¸ ë° ë°ëª¨ìš©"""
    print("=" * 70)
    print("ê°ì„± ë¶„ì„ ëª¨ë¸ ìœ í‹¸ë¦¬í‹° - ì˜ˆì‹œ ì‹¤í–‰")
    print("=" * 70)

    # ========== Word2Vec ëª¨ë¸ í…ŒìŠ¤íŠ¸ ==========
    print("\n" + "=" * 70)
    print("Word2Vec ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    print("\nëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    initialize("word2vec")

    # ì˜ˆì‹œ ë¬¸ì¥ë“¤ í…ŒìŠ¤íŠ¸
    print("\nì˜ˆì‹œ ë¬¸ì¥ ê°ì„± ë¶„ì„ ê²°ê³¼:\n")

    for i, text in enumerate(EXAMPLE_SENTENCES, 1):
        score = predict_sentiment(text, verbose=False, vectorizer_type="word2vec")
        label = get_sentiment_label(score)

        emoji = "ğŸ˜Š" if label == "ê¸ì •" else "ğŸ˜" if label == "ë¶€ì •" else "ğŸ˜"

        print(f"{i:2d}. [{emoji} {label}] {score:.3f} | {text[:50]}")

    # ========== BERT ëª¨ë¸ í…ŒìŠ¤íŠ¸ ==========
    print("\n" + "=" * 70)
    print("BERT ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    print("\nëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    initialize("bert")

    # ì˜ˆì‹œ ë¬¸ì¥ë“¤ í…ŒìŠ¤íŠ¸
    print("\nì˜ˆì‹œ ë¬¸ì¥ ê°ì„± ë¶„ì„ ê²°ê³¼:\n")

    for i, text in enumerate(EXAMPLE_SENTENCES, 1):
        score = predict_sentiment(text, verbose=False, vectorizer_type="bert")
        label = get_sentiment_label(score)

        emoji = "ğŸ˜Š" if label == "ê¸ì •" else "ğŸ˜" if label == "ë¶€ì •" else "ğŸ˜"

        print(f"{i:2d}. [{emoji} {label}] {score:.3f} | {text[:50]}")

    # ========== ë°°ì¹˜ ì˜ˆì¸¡ ë¹„êµ í…ŒìŠ¤íŠ¸ ==========
    print("\n" + "=" * 70)
    print("ë°°ì¹˜ ì˜ˆì¸¡ ë¹„êµ í…ŒìŠ¤íŠ¸ (Word2Vec vs BERT)")
    print("=" * 70)

    test_texts = [
        "ì •ë§ ì¢‹ì€ ì œí’ˆì´ì—ìš”!",
        "ì™„ì „ ì‹¤ë§í–ˆì–´ìš”...",
        "ê·¸ëƒ¥ ë³´í†µì´ì—ìš”",
    ]

    scores_w2v = batch_predict(test_texts, vectorizer_type="word2vec")
    scores_bert = batch_predict(test_texts, vectorizer_type="bert")

    print(f"\n{'í…ìŠ¤íŠ¸':<30} {'Word2Vec':<12} {'BERT':<12} {'ì°¨ì´':<8}")
    print("-" * 70)
    for text, score_w2v, score_bert in zip(test_texts, scores_w2v, scores_bert):
        label_w2v = get_sentiment_label(score_w2v)
        label_bert = get_sentiment_label(score_bert)
        diff = abs(score_w2v - score_bert)
        print(
            f"{text:<30} {score_w2v:.3f} ({label_w2v:<3}) {score_bert:.3f} ({label_bert:<3}) {diff:.3f}"
        )

    print("\n" + "=" * 70)
    print("ì‚¬ìš© ì˜ˆì‹œ:")
    print("  from model_utils import predict_sentiment")
    print("  # Word2Vec ì‚¬ìš©")
    print(
        '  score = predict_sentiment("ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”!", vectorizer_type="word2vec")'
    )
    print("  # BERT ì‚¬ìš©")
    print('  score = predict_sentiment("ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”!", vectorizer_type="bert")')
    print("=" * 70)


if __name__ == "__main__":
    main()
    predict_sentiment(
        "ë‚˜ëŠ” ì´ê²ƒ ì €ê²ƒ ë§ì€ ì œí’ˆì„ ì¼ëŠ”ë° ê·¸ ì¤‘ì—ì„œ ì œì¼ ì¢‹ë‹¤",
        verbose=True,
    )
    predict_sentiment(
        "ë‚˜ëŠ” ì´ê²ƒ ì €ê²ƒ ë§ì€ ì œí’ˆì„ ì¨ë´¤ëŠ”ë° ëŒ€ë¶€ë¶„ ë³„ë¡œ ì˜€ì–´ ê·¸ëŸ°ë° ì´ ì œí’ˆì€ ë§¤ìš° ì¢‹ì•„",
        verbose=True,
    )
